Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job stats:
job                               count
------------------------------  -------
all                                   1
combine_chr_update_coordinates        1
filter_vcf                            1
genotype_all                          1
map_reads                             1
mark_duplicates                       1
variant_calling                       1
total                                 7

Select jobs to execute...

[Sun Sep 14 19:53:28 2025]
rule mark_duplicates:
    input: scripts/02_mark_duplicates_sktest.sh
    output: output/mark_duplicates_done.txt
    jobid: 3
    reason: Missing output files: output/mark_duplicates_done.txt
    resources: tmpdir=/tmp


[Sun Sep 14 19:53:28 2025]
rule genotype_all:
    input: scripts/05_genotype_all.sh
    output: /home/hamzaamhal/snakemake_pipline2/output/genotype_all_done.txt
    jobid: 5
    reason: Missing output files: /home/hamzaamhal/snakemake_pipline2/output/genotype_all_done.txt; Set of input files has changed since last execution; Code has changed since last execution
    resources: tmpdir=/tmp

[Sun Sep 14 19:53:28 2025]
Error in rule mark_duplicates:
    jobid: 3
    input: scripts/02_mark_duplicates_sktest.sh
    output: output/mark_duplicates_done.txt
    shell:
        
        nohup bash scripts/02_mark_duplicates_sktest.sh > /home/hamzaamhal/snakemake_pipline2/logfile/mark_duplicates_output.log 2>&1
        touch output/mark_duplicates_done.txt
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

